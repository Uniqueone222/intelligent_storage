# Intelligent Storage System - Quick Reference Guide

## File Locations & Key Code

### 1. Database Models (5 Models)
**File**: `backend/storage/models.py`

```python
class MediaFile(models.Model):
    """Uploaded file metadata with AI analysis"""
    original_name, file_path, file_size
    detected_type, mime_type, file_extension, magic_description
    ai_category, ai_subcategory, ai_tags, ai_description
    user_comment
    storage_category, storage_subcategory, relative_path
    uploaded_at, processed_at

class JSONDataStore(models.Model):
    """Structured JSON data tracking"""
    name, description, database_type, confidence_score
    table_name, collection_name
    inferred_schema, sample_data
    structure_depth, has_nested_objects, has_arrays, is_consistent
    ai_reasoning, user_comment, record_count
    created_at, updated_at

class UploadBatch(models.Model):
    """Batch upload tracking"""
    batch_id, total_files, processed_files, failed_files
    status (processing/completed/failed)
    started_at, completed_at

class DocumentChunk(models.Model):
    """RAG: Document chunks with vector embeddings"""
    media_file (FK), chunk_index, chunk_text, chunk_size
    embedding (VectorField, 768 dimensions)
    file_name, file_type, page_number, metadata
    created_at, updated_at

class SearchQuery(models.Model):
    """Search history with embeddings"""
    query_text, query_embedding (768 dimensions)
    results_count, top_result_score
    user_session, ip_address, created_at
```

---

### 2. API Endpoints (11 URL Patterns)
**File**: `backend/storage/urls.py`

```python
# ViewSet routes (generated by router)
router.register(r'media-files', MediaFileViewSet)
router.register(r'json-stores', JSONDataViewSet)

# Upload endpoints
POST   /api/upload/file/           # Single file
POST   /api/upload/batch/          # Multiple files
POST   /api/upload/json/           # JSON data

# Media files
GET    /api/media-files/           # List
GET    /api/media-files/by_category/
GET    /api/media-files/statistics/
GET    /api/media-files/{id}/

# JSON stores
GET    /api/json-stores/           # List
GET    /api/json-stores/{id}/query/
GET    /api/json-stores/list_databases/

# RAG/Semantic Search
POST   /api/rag/index/{file_id}/   # Index document
POST   /api/rag/search/            # Semantic search
POST   /api/rag/query/             # Q&A with sources
POST   /api/rag/reindex-all/
GET    /api/rag/stats/

# System
GET    /api/health/                # Health check
```

---

### 3. File Type Detection
**File**: `backend/storage/file_detector.py`

```python
class FileTypeDetector:
    FILE_TYPE_CATEGORIES = {
        'images': {
            'extensions': ['.jpg', '.jpeg', '.png', '.gif', '.bmp', ...],
            'mime_prefixes': ['image/'],
            'magic_patterns': ['image data', 'bitmap', 'JPEG', ...]
        },
        'videos': {...},
        'audio': {...},
        'documents': {...},
        'compressed': {...},
        'programs': {...},
    }
    
    def detect_file_type(file_path):
        # Multi-layer scoring:
        # - Magic bytes (weight: 3) - most reliable
        # - MIME type (weight: 2)
        # - File extension (weight: 1)
        # Returns: (category, metadata_dict)

# Usage:
file_detector = FileTypeDetector()
category, metadata = file_detector.detect_file_type('/path/to/file')
subcategory = file_detector.get_subcategory_suggestion(file_path, ai_analysis)
```

---

### 4. AI-Powered Analysis
**File**: `backend/storage/ai_analyzer.py`

```python
class OllamaAnalyzer:
    def __init__(self):
        self.host = settings.OLLAMA_SETTINGS['HOST']  # http://localhost:11434
        self.model = settings.OLLAMA_SETTINGS['MODEL']  # llama3:latest
    
    def analyze_image(image_path, user_comment):
        # Calls Ollama API with llama3.2-vision
        # Returns: {category, tags, description}
    
    def analyze_file_content(file_path, file_type, user_comment):
        # Calls Ollama API with llama3
        # Returns: {category, tags, description}
    
    def analyze_json_for_db_choice(json_data, user_comment):
        # Analyzes structure depth, nesting, arrays
        # Returns: {database_type, confidence, reasoning, suggested_schema}

# Usage:
ai_analyzer = OllamaAnalyzer()
result = ai_analyzer.analyze_image('photo.jpg', 'vacation photo')
# Returns: {'category': 'landscape', 'tags': [...], 'description': '...'}
```

---

### 5. Database Manager (SQL + NoSQL)
**File**: `backend/storage/db_manager.py`

```python
class DatabaseManager:
    def __init__(self):
        self.mongo_client = MongoClient(connection_string)
        self.mongo_db = self.mongo_client[db_name]
    
    def store_json_data(data, db_type, collection_name, table_name):
        # Routes to _store_in_mongodb() or _store_in_postgresql()
        # Returns: {success, table/collection name, schema, records_count}
    
    def _store_in_postgresql(data, table_name, user_comment):
        # Generates schema from data structure
        # Creates table: CREATE TABLE table_name (...)
        # Inserts rows: INSERT INTO table_name VALUES (...)
        # Returns: {success, table, schema, sql_schema}
    
    def _store_in_mongodb(data, collection_name):
        # Generates schema from sample document
        # Creates collection if not exists
        # Inserts documents: collection.insert_many(data)
        # Returns: {success, collection, mongo_schema}
    
    def query_mongodb(collection_name, limit=100):
        # Returns list of documents
    
    def query_postgresql(table_name, limit=100):
        # Returns list of rows as dicts

# Usage:
db_manager = DatabaseManager()
result = db_manager.store_json_data(
    data=[{'name': 'John', 'age': 30}],
    db_type='SQL',
    table_name='users'
)
```

---

### 6. RAG Services (Semantic Search)
**File**: `backend/storage/rag_service.py`

```python
class RAGService:
    def index_document(media_file):
        # 1. Extract text from file
        # 2. Chunk text into 500-char pieces
        # 3. Generate embedding for each chunk (Ollama)
        # 4. Store DocumentChunk records in PostgreSQL
        # Returns: {success, chunks_created, file_name}
    
    def search(query, limit=10, file_type_filter=None):
        # 1. Generate embedding for query
        # 2. Vector search: embedding <-> query_embedding
        # 3. Return top K similar chunks
        # Returns: [{chunk_text, similarity_score, file_name, ...}, ...]
    
    def generate_rag_response(question, max_context_chunks=5, file_type_filter=None):
        # 1. Search for relevant chunks
        # 2. Prompt Ollama with question + context
        # 3. Generate AI response with source citations
        # Returns: {success, answer, sources, reasoning}
    
    def reindex_all_documents():
        # Reindex all MediaFile records
        # Useful after schema changes
        # Returns: {total_indexed, chunks_created, errors}

# Usage:
rag_service = RAGService()
result = rag_service.index_document(media_file_instance)
results = rag_service.search(query='machine learning', limit=5)
answer = rag_service.generate_rag_response(
    question='What is this about?',
    max_context_chunks=3
)
```

---

### 7. Embedding Service
**File**: `backend/storage/embedding_service.py`

```python
class EmbeddingService:
    def __init__(self):
        self.ollama_host = settings.OLLAMA_SETTINGS['HOST']
        self.embedding_model = 'nomic-embed-text'  # 768 dimensions
    
    def generate_embedding(text: str) -> List[float]:
        # Calls: POST {host}/api/embeddings
        # Returns: List of 768 floats
    
    def generate_embeddings_batch(texts: List[str]) -> List[List[float]]:
        # Batch embedding generation
        # Returns: List of embedding vectors
    
    def ensure_model_available() -> bool:
        # Check if embedding model is available
        # Returns True/False

# Usage:
embedding_service = EmbeddingService()
vector = embedding_service.generate_embedding('hello world')
# Returns: [0.234, 0.567, ..., 0.891]  # 768 dimensions
```

---

### 8. Chunking Service
**File**: `backend/storage/chunking_service.py`

```python
class ChunkingService:
    def __init__(self, max_chunk_size=500, overlap_size=50):
        self.max_chunk_size = max_chunk_size
        self.overlap_size = overlap_size
        self.separators = ['\n\n', '\n', '. ', ' ', '']  # Try in order
    
    def chunk_text(text: str, metadata: Dict) -> List[Dict]:
        # Splits text into chunks with separators
        # Returns: [{chunk_index, chunk_text, chunk_size, metadata}, ...]
    
    def extract_text_from_file(file_path, file_type):
        # Extracts text from various file types
        # Supports: PDF, DOCX, TXT, MD, HTML, etc.
        # Returns: String of extracted text

# Usage:
chunking_service = ChunkingService(max_chunk_size=500)
chunks = chunking_service.chunk_text(
    text='Long document text...',
    metadata={'file_type': 'pdf', 'source': 'report.pdf'}
)
# Returns: [
#   {'chunk_index': 0, 'chunk_text': '...', 'chunk_size': 450, 'metadata': {...}},
#   {'chunk_index': 1, 'chunk_text': '...', 'chunk_size': 480, 'metadata': {...}},
# ]
```

---

### 9. Views Layer (Upload Handling)
**File**: `backend/storage/views.py`

```python
# Single File Upload
class FileUploadView(APIView):
    def post(self, request):
        # 1. Validate FileUploadSerializer
        # 2. Save temp file
        # 3. Detect file type (magic + MIME + extension)
        # 4. AI analysis (Ollama)
        # 5. Organize: move to media/{category}/{subcategory}/
        # 6. Create MediaFile record
        # 7. Return 201 + file data

# Batch File Upload
class BatchFileUploadView(APIView):
    def post(self, request):
        # 1. Create UploadBatch record
        # 2. Process each file (same as single)
        # 3. Track progress
        # 4. Return batch summary

# JSON Upload
class JSONDataUploadView(APIView):
    def post(self, request):
        # 1. AI analyze JSON structure
        # 2. Decide SQL or NoSQL
        # 3. Store in appropriate database
        # 4. Create JSONDataStore record
        # 5. Return 201 + storage details

# Media Files CRUD
class MediaFileViewSet(viewsets.ModelViewSet):
    queryset = MediaFile.objects.all()
    serializer_class = MediaFileSerializer
    
    @action(detail=False, methods=['GET'])
    def by_category(self, request):
        # Filter by detected_type parameter
    
    @action(detail=False, methods=['GET'])
    def statistics(self, request):
        # Return: {total_files, by_type, total_size}

# JSON Stores CRUD
class JSONDataViewSet(viewsets.ModelViewSet):
    queryset = JSONDataStore.objects.all()
    serializer_class = JSONDataStoreSerializer
    
    @action(detail=True, methods=['GET'])
    def query(self, request, pk=None):
        # Query the dataset (PostgreSQL or MongoDB)
    
    @action(detail=False, methods=['GET'])
    def list_databases(self, request):
        # List all tables and collections
```

---

## Common Operations

### Upload a File
```bash
curl -X POST http://localhost:8000/api/upload/file/ \
  -F "file=@photo.jpg" \
  -F "user_comment=vacation photo"
```

### Query Media Files
```bash
# List all
curl http://localhost:8000/api/media-files/

# Filter by type
curl http://localhost:8000/api/media-files/?detected_type=images

# Get stats
curl http://localhost:8000/api/media-files/statistics/
```

### Upload JSON Data
```bash
curl -X POST http://localhost:8000/api/upload/json/ \
  -H "Content-Type: application/json" \
  -d '{
    "data": [
      {"id": 1, "name": "John", "email": "john@example.com"}
    ],
    "name": "users",
    "force_db_type": "SQL"
  }'
```

### Semantic Search
```bash
curl -X POST http://localhost:8000/api/rag/search/ \
  -H "Content-Type: application/json" \
  -d '{
    "query": "machine learning",
    "limit": 10
  }'
```

### RAG Query (Q&A)
```bash
curl -X POST http://localhost:8000/api/rag/query/ \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What is the main topic?",
    "max_sources": 5
  }'
```

---

## Environment Variables

### PostgreSQL
```
POSTGRES_NAME=intelligent_storage_db
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres123
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
```

### MongoDB
```
MONGODB_HOST=localhost
MONGODB_PORT=27017
MONGODB_USER=admin
MONGODB_PASSWORD=admin123
MONGODB_DB=intelligent_storage_nosql
```

### Ollama
```
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3:latest
```

### Django
```
DJANGO_SECRET_KEY=your-secret-key-here
DJANGO_DEBUG=True
DJANGO_ALLOWED_HOSTS=localhost,127.0.0.1
```

---

## Database Schemas

### MediaFile Table
```sql
CREATE TABLE storage_mediafile (
    id BIGSERIAL PRIMARY KEY,
    original_name VARCHAR(255),
    file_path VARCHAR(1024),
    file_size BIGINT,
    detected_type VARCHAR(50),  -- images, videos, etc.
    mime_type VARCHAR(100),
    file_extension VARCHAR(20),
    magic_description TEXT,
    ai_category VARCHAR(255),
    ai_subcategory VARCHAR(255),
    ai_tags JSONB,
    ai_description TEXT,
    user_comment TEXT,
    storage_category VARCHAR(100),
    storage_subcategory VARCHAR(100),
    relative_path VARCHAR(1024),
    uploaded_at TIMESTAMP,
    processed_at TIMESTAMP,
    -- Indexes
    INDEX(detected_type),
    INDEX(storage_category),
    INDEX(uploaded_at)
);
```

### DocumentChunk Table (RAG)
```sql
CREATE TABLE storage_documentchunk (
    id BIGSERIAL PRIMARY KEY,
    media_file_id BIGINT,  -- FK to MediaFile
    chunk_index INT,
    chunk_text TEXT,
    chunk_size INT,
    embedding vector(768),  -- pgvector
    file_name VARCHAR(255),
    file_type VARCHAR(50),
    page_number INT,
    metadata JSONB,
    created_at TIMESTAMP,
    updated_at TIMESTAMP,
    -- Indexes
    INDEX(file_name),
    INDEX(file_type),
    INDEX(created_at)
);
```

### JSONDataStore Table
```sql
CREATE TABLE storage_jsondatastore (
    id BIGSERIAL PRIMARY KEY,
    name VARCHAR(255) UNIQUE,
    description TEXT,
    database_type VARCHAR(10),  -- SQL or NoSQL
    confidence_score INT,
    table_name VARCHAR(255),
    collection_name VARCHAR(255),
    inferred_schema JSONB,
    sample_data JSONB,
    structure_depth INT,
    has_nested_objects BOOLEAN,
    has_arrays BOOLEAN,
    is_consistent BOOLEAN,
    ai_reasoning TEXT,
    user_comment TEXT,
    record_count INT,
    created_at TIMESTAMP,
    updated_at TIMESTAMP,
    -- Indexes
    INDEX(database_type),
    INDEX(created_at)
);
```

---

## File Organization Structure

```
media/
├── images/
│   ├── PNG/                      # Default subcategories
│   ├── JPG/
│   ├── GIF/
│   └── nature/                   # AI-generated subcategories
│       └── 20241115_143022_landscape.jpg
├── videos/
│   ├── MP4/
│   └── tutorials/
│       └── 20241115_143022_guide.mp4
├── documents/
│   ├── pdf/
│   └── word/
├── audio/
│   ├── MP3/
│   └── WAV/
├── compressed/
│   └── archives/
├── programs/
│   ├── windows/
│   ├── scripts/
│   └── executables/
├── others/
└── temp/                         # Temporary upload directory
```

---

## Response Examples

### File Upload Success
```json
{
  "success": true,
  "file": {
    "id": 1,
    "original_name": "photo.jpg",
    "file_path": "/home/user/media/images/nature/20241115_143022_photo.jpg",
    "file_size": 2048576,
    "detected_type": "images",
    "mime_type": "image/jpeg",
    "file_extension": ".jpg",
    "magic_description": "JPEG image data, JFIF standard 1.01",
    "ai_category": "landscape",
    "ai_subcategory": "nature",
    "ai_tags": ["mountain", "sunset", "landscape"],
    "ai_description": "Beautiful mountain landscape during sunset",
    "user_comment": "vacation photo",
    "storage_category": "images",
    "storage_subcategory": "nature",
    "relative_path": "images/nature/20241115_143022_photo.jpg",
    "uploaded_at": "2024-11-15T14:30:22.123456Z",
    "processed_at": "2024-11-15T14:30:22.123456Z"
  },
  "message": "File uploaded and organized in images/nature/"
}
```

### JSON Upload Success
```json
{
  "success": true,
  "storage": {
    "id": 1,
    "name": "users",
    "database_type": "SQL",
    "confidence_score": 85,
    "table_name": "users",
    "record_count": 2,
    "created_at": "2024-11-15T14:35:10.123456Z"
  },
  "ai_analysis": {
    "database_type": "SQL",
    "confidence": 85,
    "reasoning": "Consistent structure, simple fields, good for relational storage"
  },
  "generated_schema": {
    "type": "SQL",
    "table_name": "users",
    "create_statement": "CREATE TABLE users (id INT, name TEXT, email TEXT)",
    "columns": {...}
  },
  "message": "Data stored in SQL database"
}
```

### Semantic Search Results
```json
{
  "success": true,
  "query": "machine learning",
  "results_count": 3,
  "results": [
    {
      "chunk_text": "Machine learning is a subset of artificial intelligence...",
      "similarity_score": 0.92,
      "file_name": "ml_guide.pdf",
      "file_type": "documents",
      "chunk_index": 2
    },
    {
      "chunk_text": "Deep learning uses neural networks for machine learning tasks...",
      "similarity_score": 0.88,
      "file_name": "ai_handbook.pdf",
      "file_type": "documents",
      "chunk_index": 5
    }
  ]
}
```

### Health Check
```json
{
  "status": "healthy",
  "services": {
    "django": true,
    "postgresql": true,
    "mongodb": true,
    "ollama": true
  }
}
```

---

## Key Integration Points

### To Add Authentication:
1. Update `core/settings.py`:
   - Add `REST_FRAMEWORK['DEFAULT_AUTHENTICATION_CLASSES']`
   - Add `REST_FRAMEWORK['DEFAULT_PERMISSION_CLASSES']`

2. Update views with permission classes:
   ```python
   class FileUploadView(APIView):
       permission_classes = [IsAuthenticated]
   ```

### To Add User Isolation:
1. Add `user` FK to MediaFile and JSONDataStore
2. Override `queryset` in ViewSets:
   ```python
   def get_queryset(self):
       return self.queryset.filter(user=self.request.user)
   ```

### To Add Async Processing:
1. Create Celery task in `storage/tasks.py`
2. Call from views:
   ```python
   from .tasks import process_file_async
   task = process_file_async.delay(file_id)
   ```

---

**For more details, see ARCHITECTURE_OVERVIEW.md**
